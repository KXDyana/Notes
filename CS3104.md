# CS3104

 ## Introduction:

OS: program that manages computer hardware, also provides a basis for application programs as an intermediary between user and hardware.

User <--> Application/System <--> OS <--> Hardware(resource) 

### Functions of OS:
* interface
* resource allocation
* management of memory, security, etc.

### Main types of OS:
* Batch OS
* Time sharing OS
* Distributed OS
* Network OS
* Real Time OS 
* Multi programming/processing/tasking OS

### Goals of OS:
* Convenience
* Efficiency
* Both (most OS are designed for) 

## Basics

* A modern general-purpose computer system consists of one or more CPUs and a number of device controllers connected through a common bus that provides access to shared memory
* Each device controller is in charge of a specific type of device
* the CPU and the device conrollers can execute concurrently.
*  to ensure orderly access to the shared memory, there need to be a memory controller

### Important Terms
* Bootstrap Program: the initial program that runs when a computer is powered up or rebooted. 
    * stored in ROM
    * must know how to load the OS 
    * must locate and load the OS kernel into memory
* Interrupt: The occurence of an event is usually signalled by an interrupt from a hardware of software.
    * by sending a signal to the CPU (by using the system bus).
* System Call (Monitor Call): software triggers interrupt by using system calls.

When the CPU is interrupted, it stops what it is doing at the moment and immediately transfers execution to a fixed location (where contains the starting address where the service routine of the calling interrupt is located). On completion, the CPU resumes the interrupted computation. 



## Storage Structure

Registers
Cache
Main Memory
Electronic Disk
Magnetic Disk
Optical Disk
Magnetic Tapes

* Volatile: when power is removed, the contents stored loses
* Non-Volatile: retains its contents even when power is removed.

## I/O Structure

* I/O: giving input/getting output
* storage is only one of the many types of I/O devices within a computer
* A modern general-purpose computer system consists of multiple device controllers. Each device controller is in charge of a specific type of device.
* A device controller contains a local buffer storage and a set of specia purpose registers.
* OS have a device driver for each device controller. This driver understands the device controller and persents a uniform interface to the device to the rest of the OS.

### Working of an I/O Operation:
* the device driver loads the registers given by the device controller
* the device controller examines the contents ofthe registers to determine what action to take
* the controller starts to transfer data from the device to its buffer
* the controller informs the driver via an interrupt the the operation is finished
* the driver returns control to the OS.

This form of interrupt-driven I/O is fine for moving small amounts of data, but can produce high overhead when used for large data movement. To solved this, Direct Memory Access (DMA) is used.

* DMA: after setting up buffers, pointers, and counters for I/O device, the controller transfers an entire block of data directly to or from its own buffer storage to memory without interving CPU.

## Computer System Architecture
Types of computer systems based on the number of general purpose processors:
1. single process systems: one main CPU capable of executing a general purpose instruction set including instructions from user processes. Other special purpose processors are also present which perform device specific tasks (such as in a keyboard).
2. multiprocesor systems: also known as parallel systems or tightly coupled systems. Two or more processors in  close communication, sharing the computer bus and sometimes the clock, memory and devices.
    * increased throughput
    * economy of scale
    * increased reliability
3. clustered systems

## Structure

* multiprogramming: increases CPU utilisation by organiszing jobs so that CPU always has one to execute. 
* time sharing: CPU executes multiple jobs by switching among them. The switching occur frequently that the user can interact with each program while it is running.
    * each user has at least one separate program in memory
    * a program loaded into memory and executed is called a process

## Services:

An OS provides an enviroment for applications and programs.


* Command line Interface
* Graphical User Interface

* Program Execution

* I/O Operations

* File System Manipulation

* Communications (between processes)

* Error Detection

* Resource Allocation

* Accounting (keep track of usages)

* Protection and Security


## Interfaces:

### Command Line Interface (Command Interpreter)
allow users to directly enters commands that are to be performed by the OS
* some OS includes the CLI in the kernel. others such as windowsXP/UNIX, CLI is treated as a special program. 
* on some systems with multiple CLI to choose from, the CLI is know as shells. e.g.: bourne shell, C shell, Bourne-again shell(BASH), korn shell, etc. 
    * 1. the code of certain tasks (such as creating a file) is contained in the command interpreter.
    * 2. or,  the interpreter calls the program that can perform the task based on the command.


## System Calls:

System Calls provide an interface to the services made available by an OS.
System call is the programmatic way the program requests a service from the kernel of the OS. Available as routines written in C or C++

### Types of System Calls:

1. Process control
    * load and execute
    * end (halted) and abort when error occurs
    * create process, terminate process
    * get process attributs, set process attributes
    * wait for time
    * wait events, signal event
    * allocate and free memory

2. File Manipulation
    * create file and delete file
    * open and close file
    * read, write and reposition file
    * get and set file attributes

3. Device Management
    * request devices, release device
    * read, write, reposition
    * get and set device attributes
    * logically attach or detach devices

4. Information Maintenance
    * get and set time/date
    * get and set system data
    * get and set process, file and device attributes
    
5. Communications
    * create, delete connections between processes
    * send and recieve messages
    * transfer status information
    * attach or detach remote devices

## System Programs

 Just above Operating Systems, provides a convenient environment for program development and execution. e.g. user interfaces to system calls or gui.

### File Management
* Create, Delete, Copy, Rename, Print, Dunp, List, General Manipulation

### Status Information 
* Ask the System for Date, time, amount of available memory or disk storage, number of users, performance, logging and debugging info, etc.

### File modification
* (built in) text editors
*  search contents of files or transformations of text

## Programming Language support
* Compilers, Assemblers, Debugers and Interpreters for 
* such as C/C++/Java/Visual Basic/PERL

## Program Loading and Execution
* absolute loaders
* relocatable loaders
* linkage editors
* overlay loaders
* debugging systems for high level languages or machine language are needed are well

## Communications 
* creating virtual connections among processes, users and computer systems
* allow users to send messages to another's computer/screens
* browse webpages
* send messages
* log in remotely

## Others
* web browers
* text processors
* spreadsheets
* database systems
* games
   ...


## Design and Inplementation

Design Goals:
Defining goals and Specification
    * Choice of Hardware
    * Types of System

Requirements: 
    * User Goals (easy to use)
    * System Goals (easy to develop and maintain)
    * Mechanism and Policies: mechanisms determine how, policies determine what

Inplementation
* traditionally, OS have been written in assembly
* nowadays, mostly written in C/C++
    therefore, easier to port on different hardwares, understand, debug, more compact, and can be written faster.

## Structure of OS

* Simple Structure:

    ```
    Application Programs
    Resident System Programs
    Device Drivers
    ROM BIOS device drivers (globally accessiable)
    ```
    prone to malicious programs and crashes
    Intel 8088 has no hardware protection

* Monolithic Structure:

    ```
    Users

    Shells and command lines
    Comilter and Interpreters
    System Libraries

    System Call Interface

    Signal, terminal handling, I/O, CPU scheduling...

    terminal controllers, device controllers, memory controllers
    ```
    too many things packed into one level
    difficult to debug

* Layered Structure
    ```
    Layer N: UI

    .
    .
    .
    Layerj 0: Hardware
    ```
    A layer can only use those that are below (through System Calls)
    Not very efficient

* Microkernels

    * kernel only provide system calls
    * all other functionalities are implemented in User mode as seperate applications.
    * suffers from performance due to heavy system overhead

* Modules

    * Object Oriented Approaches
    * Different would be loaded to the kernel when required
    * Each module has a defined protective interface#
    * Each module can directly communicate to another module (not like layered)
    * Modules are dynamically loaded to kernel, so less overhead (compared to microkernels)


## Virtual Machines

Abstract the hardware of a single ocmputer into several different execution enviroments, thereby creating the illution that each seperate exeuction environment is running its own private computer

* Virtual Machine Software: Kernel Mode
* Virtual Mahcine itself: User mode

A virtual machine also has a virtual user mode and a virtual kernel mode (both in user mode)

## OS Generation & System Root

1. design, code and implement an OS for one machine
2. OS to run on any of a class of machines at a variety of machines
3. OS be configured or generated for each specific computer site, where System Generation (SYSGEN) is used
    SYSGEN:
    * what CPU?
    * how much memory?
    * what devices?
    * what OS options are desired?

 ### System Boot
The procedure of starting a computer by loading the kernel.
    * done by a small piece of code known as bootstrap, which locates the kernel
    * bootstrap is in the form of read-only memory (ROM) because RAM is in an unknown state at system startup. ROM needs no initialization and cannot be infected by virus (cannot be modified).

### Firmware
 EPROM: erasable programmable ROM, can be used to update bootstrap and firmware.

When the full bootstrap program is loaded, it can traverse the file system to find the OS kernel and load it into memory, and start its execution. It is only then the system is said to be running




## Process Management

### Process and Threads
* Process: a program in execution
* Thread: the unit of execution within a process    

### Process State
* As process exeuctes, it changes state, defined by current activity

1. New: process created
2. Running: Instructions executed
3. Waiting: process waiting for event to occur
4. Ready: process waiting to be assigned to processor
5. Terminated: process finished execution

## Process Control Block (PCB) (Task Control Block)

```
Process ID (Process number)
Process State
Process counter (address of the next line of instructor)
Registers (been used by this process)
Memory Limits
List of open files
Accounting Info
I/O status Info
...
```                                                                    
## Process Scheduling

Objective: havbe some process running at all times to maximize CPU utilization

* Job Queue: when a process enters the system, they are put into the job queue, consists of all processes in the system.
* Ready Queue: the processes that are residing in main memory and are ready and waiting to be executed

### Context Switch
* Interrupts cause the OS to change a CPU from its current task to run a kernel routine
* When an interrupt occurs, the system  needs to save the current context of the process so that it can be restored. 
* The context is represented in the PCB of the process.
* switching the CPU to another process requires performing a state save of the current process and a state restore of a different process
* context-switch is pure overhead (cost) because the system does no useful work during switching. speed varies from machine to machine and depend on various factors. typically are a few milliseconds.

## Process Creation

a process may create several new processes via a create-process system call. Each children process can in turn create other processes, forming a tree of processes

When new process is created, Two possibilities:
1. the parents continues to execute concurrently with its children
2. the parents waits until some or all of its children have terminated

for address space, Two possibilities:
1. the child is a duplicate of the parent process(same program and data)
2. the child has a new program loaded into it

## Process Termination
* A process terminates when it finishes executing its final statement and asks the operating system to delete it by using the exit() system call.
    * At that point, the process may return a status value (typically int) to its parent process (via the wait() system call)
    * all the resources of the process, including physical and virtual memory, open files, and I/O buffers, are deallocated by the OS.
* A process can also cause the termination of another process via an appropriate system call.
    * usually, such a system call can only be invoked by the parent of the terminated process
    * otherwise, users could arbitrarily kill each other's jobs
    * Reasons to terminate a child: 
        1. the child has exceeded its usage. (via)
        2. the task of the child is no longer required.
        3. the parent is exiting and the OS does not allow a child to continue if the parent is terminated.

## Interprocess Communication

Processes executing cocurrently in the OS may either be independent processes or cooperating processes

* Independent processes: they cannot affect or be affected by the other processes executing in the system
* Coopereating processes: can affect or be affected by the other processes executing in thge system, and would share data
    Reasons for cooperation: Information sharing, computation speedup, modularity, convenience
    2 models:
    1. Shared Memory:
    A region of memory that is shared by cooperating processes is established. processes can then exchange information by reading and writing data to the shared region.
        * shared memory region resides in the address space of the process creating the shared memory segment.
        * normally, two processes cannot access the memory block of each other. 
        * however, shared memory requires that the two processes agree to remove this restriction.
        * e.g. producer-consumer using a buffer
            * unbounded buffer: producer can always produce new items, but consumer must wait
            * bounded buffer: consumer must wait if the buffer is empty, producer must wait if the buffer is full

    2. Message Passing:
    Communication takes place by means of messages exchanged between the cooperating processes. 
        * useful in distributed environments where the communication processes may reside on different computers connected by a network
        * messages sent by a process can be either fixed or variable size
            * fixed size: system implementation is straightforward, but task of programming is more difficult
            * variable size: requires more complex system implementation, but the programming is simpler. 

        * if processes P and Q want to communicate, a communication link must exist between.

            * Naming: 
                * Direct
                    * P and Q must have a way to refer to each other and know each other's name to communicate (symmetry in adderssing) send(P, message) recieve(Q, message)
                    * or only the sender names the recipient to communicate (asymmetry in addressing). send(P, message) recieve(id, message)
                    * Both suffers from limited modularity: changing the identifier of a process may cause problem.
                * Indirect
                    * sending processes send to a mailbox, or port
                    * recieving processes reads from the port
                    * send(A, message) recieve(A, message)
                    * a linked may be associated with more than two processes.
            
            * Synchronization
                * Blocking (synchronous): the sending process is blocked until the message is recieved, the receiver blocks until a message is available
                * Nonblocking (asynchronous): the sending process send the message and resume, the receiving process receive either a valid messaeg or a null

            * Buffering
            Messages exchanged by communicating processes reside in a temporary queue buffer.
                * Zero capacity: the link cannot have any message waiting, so the sender must block until the recipient receives the message.
                * Bounded Capacity: the queue has finite length. if the queue is not full, the sender can continue execution. if the buffer if full, the sender must block until space is available in the queue.
                * Unbounded Capacity: the queue length is not bounded, so the sender never blocks.


## Sockets
Used for communication in client-server systems
* an endpoint for communication
* a pair of processes communicating over a network uses a pair of sockets.
* identified by an IP address concatenated with a port number.
* the server wait for client requests by listening to a specified port. Once a request is recieved, the server accepts a connetion from the client socket.
* servers implementing specific services listen to specific well-known ports (telnet: 23, ftp: 21, http: 80). All ports below 1024 are considered well known and reserved for specific services.
* port number is assigned by the host computer (>1024) following IP address.

## Remote Procedure Calls
a protocol that one program can use to request a service from a program located in another computer on network without understand the network
* very similar to IPC
* message based communication scheme must be used. 
* the message exchanged in RPC are well structured
* each message is addressed to a RPC daemon. each message contains an identifier of the function to execute and the parameter to pass to the function. (functions can be considered as services).
* the function is then executed as requested and output is sent back to requester.
### Issues of RPC and Solutions
1. difference in data representation on client and server machine: use a machine-independent representation of data (external data representation)
2. local procedure calls fail only under extreme circumstances, but RPC can fail or be deplicated because of network errors: the OS ensure that the message are acted on exactly once.
3. the client does not know the port number know on the server: 1. use a fixed port address 2. rendezvous mechanism (matchmaker) daemon on a fixed RPC.


## Threads
* thread ID
* program counter
* register set
* stack
thread shares with other threads belonging to the sam eprocess its code section, data section, and other OS resources.
    Benefits: 
    * responsiveness
    * resource sharing
    * economy
    * utilization of multiprocessor architectures

## Multithreading Models and Hyperthreading

### types of threads:
1. user threads: supported above the kernel and are managed without kernel support
2. kernel threads: supported and managed directly by the OS

Relationships:

1. many to one model (many user threads to one kernel thread)
    * many user threads to one kernel thread
    * thread management is done by thread library in user space, so efficient
    * -entire proess with block if a thread makes a blocking system call
    * -because only one thread can access the kernel at a time, multiple threads are unable to run in parallel.
2. one to one model
    * maps each user thread to a kernel thread
    * provides more concurrency than the many to one model: another thread can run when a thread makes a blocking system call
    * allows multiple threads to run in parallel
    * -creating a user thread requires creating the corresponding kernel thread
    * -the overhead of creating kernel threads can burden the performance of an application, most implementations of this model restricts the number of threads supported by the system.

3. many to may model
    * multiplexes many user threads to a smaller or equal number of kernel threads
    * the number of kernel threads may be specific to a particular application or a particular machine
    * developers can create as many user threads as necessary, and the corresponding kernel thread can run in parallel on a multiprocessor
    * when a thread performs a blocking syscall, another thread can be scheduled for execution

### Hyperthreading, or Simultaneous Multithreading(SMT)
allows their processor cores resources to become multiple logical processors for performance. It enables the processor to execute two threads, or sets of instructions at the same time.

## fork() , exec()

fork(): used to create a seperate, duplicate process
exec(): the program specified in the parameter will replace the entire process, including all threads. same pid will be used.

## Threading Issues

1. does the new process duplicate all the threads or only the thread calling fork()? : two versions
    * When to use when?
    If exec() is called immediately after forking, then duplicating only the calling thread is appropiate (the others are going to be replaced anyways)
    If the process does not call exec() after forking, all threads should be duplicated
2. Thread cancelation: a thread can get canceled before completion (target thread): two schenarios
    1. asynchronous cancellation: one thread immediately terminates the taret thread
    2. deferred cancellation: target thread periodically checks whether it should terminate.

    Difficulties: 
    * resoures allocated to a canceled thread
    * thread is canceled while updating data

    OS often reclaim system resources from canceled thread but will not reclaim all resources.
    Therefore, 2.deferred cancellation is a better approach.

## CPU Scheduling

by switching the CPU among processes, OS can make the computer more productive.

When a process is waiting for I/O event, the CPU just sits idle. the this waiting time is wasted and no useful work is accomplished.
 (CPU execution Burst vs I/O wait Burst)

* Scheduler: scheduler selects a process from the ready proesses
* Dispatcher; gives control of the CPU to the selected process. has to be quick. the time it takes for the dispatcher to stop one process and start running another is known as dispatch latency

Decision happens during the following four circumstances:
1. when a process switches from running state to waiting state
2. when a proceess switches from running state to ready state (interrupt)
3. when a process switches from waiting state to ready state (completion of I/O)
4. process terminates

1,4: non-preemptive
1,2,3,4: preemptive


### Criterias: 

#### CPU Utilisation
the CPU should be kept as busy as possible. conceptually, CPU utilizatino can range from 0% to 100%. usually, it range from 40% tp 90%.

#### Throughput
the number of work that are completed per time unit.

#### Turnaround Time
the sum of time spent waiting to get into memory, waiting in the ready queue, executing on the CPU and doing I/O

#### Waiting Time
the sum of the time spent waiting in the ready queue

#### Response Time
the time it takes to start responding



## Process Synchronization

* Cooperating process: can be affected of affect other processes. can:
    1. share address space
    2. share data through messages
when sharing address space, the data consistency need to be maintained by orderly execution of cooperating processes
* buffer: ersides in a region of memory. a producer can produce while a consumer is concuming another item. the producer and consumer must be synchronized so that the consumer does not try to consume an item that has not been produced yet.
    * Unbounded buffer: consumer have to wait for new items, but the producer can always produce
    * Bounded buffer: consumer must wait if the buffer is empty, producer must wait if the buffer is full
when several processes access and manipulate the same data concurrently and the outcome of the execution depends on the order the access takes place is called race condition.
Hence we need process synchronization.

### Critical Section
critical section: a code segment in which the process may be changing common variables, updating a table, writing a file... when a process is executing in critical section, no other process is to be allowed to execute in its critical section (no two process can execute in their critical sections at the same time). This way processes can cooperate.
#### Rules
1. each process must request permission to enter its critical section
2. the section of code implementing this request is called entry section
3. the critical section is followed by an exit section
4. the maining code is the remainder section.

#### Requirements
1. Mutual exclusion: if process is executing in its critical section, no other processes can be executing in its critical section
2. progress: if no process is executing in its critical section and some processes wish to enter their critical sections, then only those processes that are not executing in their remainder section can participate in the decision on which will enter its criticl section next, and this selection cannot be postponed indefinately
3. bounded waiting: there is a bound, or limit, on the number of times that other processes are allowed to enter their critical section after a process has made a request to enter its ciritcal section and before that request is granted.

#### Peterson's Solution
* a classic solution, software based
* may not work on modern computers, but provides a good algorithmic description of solving the critical section problem and illustrates some of the complexities.
* restricted to two processes that alternate execution between their critical sections and remainder sections.

```C
do{
    flag [iW] = true;
    turn = j;
    while (flag [j] && turn == [j]);    //if true, infinite loop

    critical section...

    flag [i] = false;

    remainder section...
}while (TRUE)
```

#### Test and Set Lock
* hard ware solutino to the synchronization problem
* there is a shared lock bariable which can take 0 or 1
* before entering into the critical section, a progress inquiries the lock
* if it is locked, it keep waiting until it is free. else, it takes the lock and execute the critical section.

```C
boolean TestAndSet (boolean *target){
    boolean rv = * target;
    *target = TRUE;
    return rv;
    //atomic operation, a single operation that cannot be disrupted
}
```
```C
do{
    while(TestAndSet(&lock));
    //do nothing
    //critical section
    lock = FALSE;   //reset the lock
    //remainder section
}while(TRUE);
```

* pros: satisfied mutual-exclusion
* cons: does not satisfy bounded-waiting


#### Semaphores

* proposed by edsger dikstra. used to manage concurrent processes by using a simple integer value
* semaphore is simply a variable which is non-negative and shared between threads. can solve critical section problem and achieve process synchronization in multiprocessing environment.
* semaphore S is an integer variable. apart from initialization, it is only accessed through two standard tatomic operations: wait(), signal()

* wait() -> P (proberen (to test))

```C
P(Semaphore S){
    while(S <= 0);  //if some process is already accessing critical section, do nothing and wait
    S--;    //if accessing, decrement S
}
```
* signal() -> V(berhogen (to increment))

```C
V(Semaphore S) {
    S++;    //signal that I have completed using the ciritcal section
}
```

* all the modification to S must be executed indivisibly (when one process modifies S, no other operation can simutaneous modify)

##### Types of Semaphore:
1. binary semaphore (mutex locks): S range only between 0 (using) and 1 (ready)
2. counting semaphore: range over an unrestricted domain, can control access to a resource that has multiple instances

#### Disadvantages of Semaphores:
* main disadvantage: busy waiting: all waiting processes muct loop continuously in the entry code. wastes CPU cycles that other processes might be able to use productively. Also called "spinlock".
* to overcome budy waiting, wait() and signal() can be modified.
* rather than engaging in busy waiting, the process can block itself: the block operation places a process into a waiting queue associated with the semaphore, and the state of the process is switched to waiting state. Then, the scheduler takes control and select another process to execute.
* can stil lead to deadlock/starvation
* deadlock: two or more processe are waiting indefinatey for an event that can be caused only by one of the waiting processes
```C
//P0
wait(S);
wait(Q);
.
.
.
signal(S);
signal(Q)
```
```C
//P1
wait(Q);
wait(S);
.
.
.
signal(Q);
signal(S)
```
* P0 and P1 are in a deadlock.


## Solving Problems Using Semaphores

### Bounded Buffer (Producer-Consumer)
* producer tries to insert data into an empty slot of buffer
* consumer tried to remove data from a filled slot of buffer
* producer must not insert when buffer is full
* consumer must not consume when the buffer is empty
* cannot produce or consume at the same time

#### Solution: 

mutex (binray semaphore, lock)
empty (counting semaphore, # of empty slots)
full (counting semaphore, # of filled slots)

```C
// producer
do{
    wait(empty);    //wait until empty > 0 and decrement empty
    wait (mutex);   //aquire lock
    //fill buffer
    signal(mutex);  //release lock
    signal(full);   //increment full 
}while(TRUE)
```
```C
// consumer
do{
    wait(full);     //wait until full > 0 and decrement full
    wait(mutex);    //aquire lock
    //consume buffer
    signal(mutex);  //release lock
    signal(empty);  //increment empty
}while(TRUE)
```

### Readers - Writers Problem
* A database is to be shared among several concurrent processes
* some of these processes may want to read only, others may want to both read and write the database. The former: Readers; the latter: Writers
* if a writer and some other thread access the databse simultaneously, problems may occur.
* so, a writer need to have exclusive access to shared database.

mutex (1, ensure mutual exclusion when readcount is updated)
wrt (1, common toboth reader and writer processes)
readCount(not a semaphore)

```C
//writer
do{
    //request for critical seciton
    wait(wrt);
    
    //write
    
    signal(wrt);
}while(TRUE)
```
```C
//reader
do{
    wait(mutex);
    readCount++;

    if(readcnt == 1) wait(wrt);//no writer can enter if there is one reader
    signal(mutex); other readers can access

    //rea
    wait(mutex);
    readcnt--;
    if (readcnt == 0) signal(wrt); //if no reader is present, writer can present
    signal(mutex);
}while(TRUE)
```


### Dinning - Philosophers Problem
* Philosophers: either thinking or eating
* while eating, a philosopher need to use two chopsticks
* there are n philosphers and n chopsticks
* one can pick up the closest chopsticks; one can only pick up one chopstick at a time; one cannot pick a chopstick that is already in other's hand.

#### Solution:
each chopstick is represented by a semaphore:
when a philosopher is grabbing a chopstick, wait() is executed on the chopstick. when a philosopher finished eating, signal() is executed on the chopstick.

```C
semaphore chopstick[5];
```

```C
//philosopher
do{
    wait(chopstick[i]);
    wait(chopstick[(i+1) % 5]); //circular fashion (5 = n)

    //eat

    signal(chopstick[i]);
    signal(chopstick[(i+1) % 5]); //circular fashion
    //think
}while (TRUE)
```

* although this solution guarantees that no two neighbors are eating simultaneously, there could still be a deadlock:
    * suppose all philosophers simultaneously get hungry and grab the left chopstick at the same time. they would keep waiting indefinately.
    * solution 1: allow at most 4 (n-1) philosophers to be sitting at the table
    * solution 2: allow a philospher to pick up the chopstick only if both are available (must pick them up in critical section)
    * solution 3: asymmetric solution: odd philosopher pick up first left and right; even philosopher pick up right first and then left.


## Monitors
* a high level abstractino that provides a convenient and effective mechanism for process synchronization. 
* a monitor presents a set of programmer defined operation that provide mutual exclusion within the monitor.

```C
monitor monitor_Name{
    //shared variable declarations
    Procedure P1(){
        ...
    }
    Procedure P2(){
        ...
    }
    ...
    initialization(){
        ...
    }
}
```

* a procedure defined within a monitor can access only the locally declared variables.
* the local variables of a monitor can only be accessed by local procedures.
* this construct ensures that only one process at a time can be active within the monitor.


## File System:
Typically the most visible part of OS

1. a collection of files (each containing data)
2. a directory structure(which organises the files)

## File Attributes:

* Name
* Identifier
* Type
* Location
* Size
* Protection
* Time, Date, User Identification

## File Operations:

* Creating
* Writing
* Reading
* Repositioning within a file
* Deleting
* Truncating

## Open Files

* File pointer 
* File-open count
* Disk Location in the file
* Access Rights


Linux: everything is a file

## Types:

* Single-level: everyting has to be uniqie
* Two-level: each user has their own directory. Very secure.
* Tree-structred Directory: more like a modern directory, with root...etc. has a concept of a current directory. No concept of different users directory.
* acyclic directoy (more complicated tree directory) : no cycles, introduces shared files and folders. 

## Inplementing File Systems
purpose and functions
1. interact with a user
2. mapped onto physical storage devices
* Layers of File Systems
    application programs - logical file system - file organization module - basic file system - I/O control - devices

    ### I/O controls
    * device drivers and interrupt handlers
    * transfer information between main memory and disk system
    
    * driver: translator
        - input = retrieve block 123; 
        - output = machine instruction by hardware controller
        usually wirtes specific bit patterns to specific locations in I/O controller's memory that tells the controller which device locations to act on and what actions to take.
    
    * basic file system: issue generic commands to the appropiate device driver to R/W physical blocks on disk
        - also manage memoty buffers and caches, particularly if they becomes full.
    * file organisation module: knows about files, their logical blocks and physical blocks. 
        - translates logical block address to physical block address.
        - knows file allocation used and location of file
        - also include free space manager to keep track of unallocated blocks and provides these blocks to the file-organisation module when requested.
    * logical file system: manage meta information
        - metadata: includes all file system structures except the actual data.
        - manage the directory structure to procude the file organisation module with the information it needs, given a symbolic file name.
        - contains file control block (FCB) (an inode in most UNIX file systems) containing infomration about the file ownership, permission, locations......

    ### Implementation:
    * boot control block (per volume): first block of volume that contains info needed by the system to boot the OS. pointed to by BIOS
    * volume control block (per volune): contains volume (partition) details: # of blocks in partition; free blocks, free blocks pointers, free FCB count and pointers
    * a directory structure (per file system): includes file names and asociated inode numbers 
    * per file FCB containing details about the file.

    ### in-memory information:
    used for both file system management and performance improvement. data is loaded at mount time and updated during file system operations, andis discarded at dismount.
    * a mount table containing information about each mounted volumn (could be lost)
    * an in memory directory structure holding info about recently accessed directories
    * system wide open file table contains copy of opend file FCB
    * per-process open file table contains a pointer to the appropriate entry in system wide open file table
    * buffers hold file system blocks when read from or write to disk

    ### FCB:
    * file permissions
    * file dates (Creation, Access, Write)
    * file owner, group, ACL
    * file size
    * file data blocks or pointers to blocks

    ### File manipulation:
    #### create
    1. calls logical file system
    2. allocate new FCB
    3. read directory into memory, update it with new filename and FCB, write back to disk.
    * some OS treat directories the same as files. the field "type" indicated whether it is a directory or a file
    #### open
    1. open() passes a file name to logical file system
    2. search the system wide open file table to see if the file is already opened
    3. if the file is already opened, a per process open file entry is created pointing to existing system wide open file table
    4. if not opened, directory strucure is searched for the given file name
    5. once found, FCB is copied into a system wide open file table in memory
    6. this table stores the FCB and tracks the numbers of processes that have the file opened.
    7. an entry is made in the per process open file table with a pointer to the entry in the system wide open file table
    * some other fields are also included, such as access mode (W,R,etc...)
        ##### close
        1. per process table entry is removed, the system wide entry open count is decremented.
        2. if all users have closed the file, updated metadata is copied back to the disk based directory structure.
        3. system wide open file table entry is removed
    
    ### Virtual file systems
    
    separates file system generic operations. 

    ### directory implementation
    #### linear list
    * easy to implement, but time consuming to execute
    * linear search of entire directory to create a new file, checking no file exists with same name
    * search is slow, directory info is frequently used.
    * some OS use a cache to avoid constant re-read from disk
    * sometimes it is good to used a linear list:  if directory is not that large/ if the files inside is properly ordered, it does not suffer from long access time; if space is important, dont need huge hashtable in memory.

    #### hash table
    * much faster than linear list
    * requires a fixed size hash function
    * can use a chained-overflow hash table. hash values becomes linked list instead.

    ### Allocation methods
    #### contiguous allocation
    * each file occupies a set of contiguous blocks on the disk
    * disk addersses define a linear ordering on the disk. accessing block b+1 after block b, normally requires no head movement.
    * head movement is minimized
    * allocation define dby disk addresses and length in block units
    * if file is n blocks long and start at b, it occupies b, b+1, ..., b+n-1
    * directory entry for each file indicates the adderss of the starting block and length of the area allocated for this file
    * accessing file is easy: access to block i of a file starting at b just at b+i
    * find space for a new file is problematic: heavy external fragmentation. 
    * worse when the largest contiguous chunk is insufficient for a request. 
    * solution: copy whole disk to another disk, free all sapce and then copy back
    #### linked allocation
    * solved all problems of contiguous allocation
    * each file is a linked list of blocks
    * blocks are scattered anywhere on the disk
    * each block contains a pointer to the next block (not available to the user) (if each block is 512 bytes in size and an address is 4 bytes, user only get to use 508 bytes)
    * each directory entry has a pointer to the first block of the file. pointer can be nil for empty files
    * writing causes free space management system to find a new block and linked to end of the file
    * reading is traversing the linked list of pointers
    * problems: worse random seek time (have to traverse blocks before); pointer storage waste space; disk head movement
    * good for sequential access
    * variation: pointer to clusters instead of blocks (FAT)
        ##### File Allocation Table (FAT)
        * section at the beginning of the disk reserved for the table that has one entry for each disk block and is indexed by the block number.
        * directory entry contains block number of the first block of the file
        * table entry indexed by that block number contains the block number for next block in file
        * result in a significant number of disk head seeks, unless FAT is cached (so that the next block position is kept).
        ##### Index Allocation
        * puts all pointers together in one place called index block
        * each files has its own index block (array of disk block addersses)
        * ith entry in index block points to ith block of the file
        * when file is created, all pointers set to nil.
        * limitation: waste a block to set up table; large file requires multiple table; small file could have a table bigger than the file itself
        * but faster than FAT since no need to traverse

        * linked scheme
        * multilevel index
        * combined scheme (unix inode)

    ### free space management
    * disk space is very limited
    * free space list keeps track of free disk space
    * records all free space blocks in a bit map
        #### linked free space


## Main Memory Management

* main purpose of a computer system is to execute programs
* during execution, these programs and the data they access must be present in the main memory (RAM, random access memory) at least partically
use of main memory:
* in CPU scheduling it is showed that speed of computer's response time could be improved. to realize this increase in performance, we must keep several processes in memory (that is, to share memory)
* bookshelf: secondary memory (disk, SSD); desk: main memory

### Hardware Structure
* memory: a large array of words or bytes, each with its own address.
* CPU fetches instructions from memory according to the value of the program counter. these instructions may cause further loading
* CPU can access only registers and main memory. some machine instructins take memory addresses as arguments, but none take disk addresses
* during any instruction, any data must be in register or in memory. if not, they must be moved before before CPU access them.
* register: accessible within one cycle of CPU clock
* main memory: many cycles. processor normally need to stall since it does not have the data required to complete the instruction that is executing.
* cache: fast temporary memory, used to accommodate speed differtial.

### Protection of OS from unauthorized access
* OS must be protected from access by user processes
* user processes must be protected from one another
* protection must be provided by hardware

To achieve this, each process has a separate memory space. Two registers BASE and LIMIT is used to achieve this.

BASE: smallest legal physical memory adderss
LIMIT: size of range (inclusive)
can be loaded only by OS using special provileged instruction, executed only in kernel mode. user cannot change the value of these registers

during access: 
    CPU > base?     if not, trap to OS monitor
    CPU <= limit?   if not, trap to OS monitor
    access granted


## Adderss Binding
* usually, a program resides on a disk as a binary executable file
* to be executed, the program must be brought into memory and placed into process. during execution, the process may be moved between disk and memory during execution

Input queue -> select process -> load into memory -> execution -> termination -> memory declared free

user program go through several steps: compile, load, execution. addresses may be different during these steps:

* source program: symbolic adddress (count)
* compiler: bind the symbolic addresses to relocatable addersses (14 bytes from the begging of this module)
* linkage editor / loader: bind the relocatable addresses to absolute addresses(74014)
each binding is a mapping from one adderss space to another

If we know that user will reside starting from location R, then the generated compiler code will start at that location and extend up from there. but if the starting location changes, it need to be recompiled.

Load time:
If we do not know at compile time where the process will reside in memory, the compiler must generate relocatable code. Final binding is delayed until load time. If the starting adderss changes, we need only reload the user code to incorporate this changed value.

Execution  Time:
If the process can be moved during its execution from one memory segment to another, binding should delayed until run time. special hardware must be available for this scheme. most general purpose OS use this method.

## Logical vs. Physical Address Space

* Logical Address: adderss generated by CPU
* Physical Address: address seen by the memory unit

* compile-time and load-time address binding methods generate identical logical and physical addersse
* execution time adderss binding scheme results in different logical and physical ddresses. in this case, logical addresses is usually referred as virtual address.

logical address space: the set of all logical addresses generated by a program
physical address space: the set of all physical addresses corresponding to these logical addresses

the runtime mapping from virtual to physical addresses is done by Memory Management Unit(MMU):

### MMU
1. CPU generate logical address : 238
2. MMU add 238 to Relocation Register (15000)
3. Physical address = 15238

logical address: 0 to max
physical address: R+0 to R+ma

## Dynamic Loading

the size of a process is limited to the size of physical memory
to obtain better memory space utilization, we can use dynamic loading
* a routine is not loaded until it is called
* all routines are kept in disk in a relocatable load format
* the main program is loaded into main memory
* when a routine needs to call another routine, the calling routine first checks whether the other routine has been already loaded. if not, the relocatable linking loader is called to load the desired routine into memory and to updae the program's address tables to reflect this change
* then, the control is passed to the newly loaded routine.

pros: 
* unused routine is never loaded
* useful when large amount of code are needed to handle infrequent cases
* the portion that is loaded and used is smaller
* is designed by user, not OS

## Dynamic Linking
when program runs, apart from its own modules, it also needs to use certains system libraries.
* static linking: system language libraries are treated like any other object module and are combined by the loader into the binary program image.
    * cons: each program on a system include a copy of its language library. waste both disk space and memory space.

* dynamic linking: check to see whether the needed routine is already in memory. if not, the routine is loaded into the memory.

Dynamic Linking is similar to dynamic loading, but is postponed until execution time.

dll: dynamically linked libraries

a stub is included in the image for each library routine reference; wihch indicates how to locate the appropiate memory residentlibrary routine or how to load the library if the routine is not already present.

the next time that particular code segment is reached, the library routine is executed directly, incurring no cost for dynamic linking

## Swapping

* a process can be swapped temporarily out of memory to a backing store and then brought back into memory for continued execution. (managed and controlled by the scheduler).

* normally, a process that is swapped out will be swapped back into the same memory space it occipied previously.
* however, it depends on the addressing binding method.
* if binding is done at assembly or load time, the process cannot be easily moved to a different location.
* if binding is done at execution time, the process can be swapped into a different memory space, because the physical addresses are computed during execution time.

### Backing store:
* must be fast
* must be large enough to accommodate copied of all memory images for all users
* must provide direct access to these memory images
* the system maintains a ready queue consisting of all processes whose memory images are on the backing store or in memory and are ready to run
* when the scheduler decides to execute a process, it calls the dispatcher which checks to see whether the next process in the queue is in memory. 
* if not, and if there is no free memory, the dispatcher swaps out a process currently in memory and swaps in the desired process
* then, it reloads registers and transfers control to the selected process

## Swap Time
* the context switch time in a swapping system is fairly high (depends on transfer rate of backing store + head seeks and latency * 2 (swap in and swap out)).

* for efficient CPU utilization, we want the execution time for each process to be long relative to the swap time. Thus, in round-robin scheduling algorithm, the time quantum should be substantially larger than the swap time.

* the major part of the swap time is transfer time. the total transfer time is directly proportional to the amount of memory swapped.

* other factors: 
    * a process must be completely idle to be swapped.
    * a process may be waiting for an I/O operation when we want to swap that process to free up memory
    * if the I/O is asynchronously accessing user memory for I/O, it cannot be swapped.

## Memory Allocation
### a simple way
* memory is devided into several fixed sized partitions
* each partition contains one process
* when a partition is free, a process is selected from the input queue and is loaded into the free partition.
* when a process terminates, the partition becomes available for other processes
* if partitions are not big enough, they can be merged. but merging partitions must be contiguous.
* if no enough partition can fit the processes, a partition can be splitted (only if they are contiguous).

## Dynamic Storage Allocation Problem (Strategy)
### First Fit
   * allocate the first hole that is big enough
   * searching can start either at the beginning of the set of holes or where the previous first fit searched ended
   * stop searching as soon as a free hole that is large enough is found
### Best Fit
   * allocate the smallest hole that is big enough
   * much search the entire partition list unless the list is ordered by size
   * produce the smallest leftover hole
### Worst Fit (why would anyone use this???)
   * allocate the largest hole
   * search the entire list unless it is ordered by size
   * produces the largest leftover hole

## Fragmentation
as processes are loaded and removed from memory, the free memory space is broken into little pieces which results in fragmentation
two types:
1. external fragmentation
    * exist when there is enough total memory space to satisfy a request, but the space are not contiguous
    * large number of small holes
    * can be severe. in worst case, there could be a block of free memory (wasted) between every two processes, which could be used to run several more processes
    * solution: compaction: shuffle the memory contents to place all free memory together in one large block. (defrag)
        * but is not always possible. If relocation is static and is done at assembly or load time, this cannot be done. only possible if relocation is dynamic and is done at execution time.
2. internal fragmentation
    * when memory blocks assigned to processes are bigger than what the process actually needs
    * some memory is left unused and wasted and cannot be used by another process.
    * solution: use best-fit approach


## Paging

* a memory management scheme that permits the physical address space of a proecss to be non-contiguous.
* avoids the considerable prblem of fitting memory chunks of varying sizes onto the backing store, which most memory management schemes suffer from.

### Basic Method

* break physical memory into fixed-sized block called frames
* break logial memory into blocks of the same size called pages.
* when a process is to be executed, its pages are loaded into any available memory frames from the backing store.
* the backing store is also divided into fixed sized blocks of the same size as frames and pages.

to keep track of which page is in which frame, use:
### Page Table
every address generated by the CPU is divided into two parts:
* page number (p) index into a page table
* page offset (d) displacement within the page

* page table contains the base address of each page in physical memory
* base address combined with page offset defines the physical memory address sent to the memory unit.
* translation: (frame * page size) + offset